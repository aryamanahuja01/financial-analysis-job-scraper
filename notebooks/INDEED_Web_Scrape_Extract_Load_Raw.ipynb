{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxvIRSQqVpWO",
        "outputId": "09e8edbd-a8de-468e-982d-e5eee8f243a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bs4\n",
            "  Using cached bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Collecting beautifulsoup4 (from bs4)\n",
            "  Downloading beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting soupsieve>1.2 (from beautifulsoup4->bs4)\n",
            "  Downloading soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting typing-extensions>=4.0.0 (from beautifulsoup4->bs4)\n",
            "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Using cached bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
            "Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
            "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "Installing collected packages: typing-extensions, soupsieve, beautifulsoup4, bs4\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [bs4]\n",
            "\u001b[1A\u001b[2KSuccessfully installed beautifulsoup4-4.13.4 bs4-0.0.2 soupsieve-2.7 typing-extensions-4.13.2\n",
            "Collecting sqlalchemy\n",
            "  Downloading sqlalchemy-2.0.40-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /Users/kiperia/Documents/sample-code/arayaman-data-science/.venv/lib/python3.12/site-packages (from sqlalchemy) (4.13.2)\n",
            "Downloading sqlalchemy-2.0.40-cp312-cp312-macosx_11_0_arm64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m372.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sqlalchemy\n",
            "Successfully installed sqlalchemy-2.0.40\n",
            "Collecting requests\n",
            "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests)\n",
            "  Downloading charset_normalizer-3.4.2-cp312-cp312-macosx_10_13_universal2.whl.metadata (35 kB)\n",
            "Collecting idna<4,>=2.5 (from requests)\n",
            "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests)\n",
            "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests)\n",
            "  Downloading certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
            "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Downloading charset_normalizer-3.4.2-cp312-cp312-macosx_10_13_universal2.whl (199 kB)\n",
            "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
            "Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
            "Downloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
            "Installing collected packages: urllib3, idna, charset-normalizer, certifi, requests\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [requests]2/5\u001b[0m [charset-normalizer]\n",
            "\u001b[1A\u001b[2KSuccessfully installed certifi-2025.4.26 charset-normalizer-3.4.2 idna-3.10 requests-2.32.3 urllib3-2.4.0\n",
            "Collecting pandas\n",
            "  Using cached pandas-2.2.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (89 kB)\n",
            "Collecting numpy>=1.26.0 (from pandas)\n",
            "  Downloading numpy-2.2.5-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/kiperia/Documents/sample-code/arayaman-data-science/.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
            "Collecting pytz>=2020.1 (from pandas)\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas)\n",
            "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in /Users/kiperia/Documents/sample-code/arayaman-data-science/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Using cached pandas-2.2.3-cp312-cp312-macosx_11_0_arm64.whl (11.4 MB)\n",
            "Downloading numpy-2.2.5-cp312-cp312-macosx_14_0_arm64.whl (5.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m547.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "Installing collected packages: pytz, tzdata, numpy, pandas\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [pandas]2m3/4\u001b[0m [pandas]\n",
            "\u001b[1A\u001b[2KSuccessfully installed numpy-2.2.5 pandas-2.2.3 pytz-2025.2 tzdata-2025.2\n",
            "Collecting firecrawl\n",
            "  Using cached firecrawl-2.5.4-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: requests in /Users/kiperia/Documents/sample-code/arayaman-data-science/.venv/lib/python3.12/site-packages (from firecrawl) (2.32.3)\n",
            "Collecting python-dotenv (from firecrawl)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting websockets (from firecrawl)\n",
            "  Using cached websockets-15.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: nest-asyncio in /Users/kiperia/Documents/sample-code/arayaman-data-science/.venv/lib/python3.12/site-packages (from firecrawl) (1.6.0)\n",
            "Collecting pydantic (from firecrawl)\n",
            "  Downloading pydantic-2.11.4-py3-none-any.whl.metadata (66 kB)\n",
            "Collecting aiohttp (from firecrawl)\n",
            "  Downloading aiohttp-3.11.18-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
            "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->firecrawl)\n",
            "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp->firecrawl)\n",
            "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp->firecrawl)\n",
            "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp->firecrawl)\n",
            "  Downloading frozenlist-1.6.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (16 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->firecrawl)\n",
            "  Downloading multidict-6.4.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp->firecrawl)\n",
            "  Downloading propcache-0.3.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (10 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp->firecrawl)\n",
            "  Downloading yarl-1.20.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (72 kB)\n",
            "Requirement already satisfied: idna>=2.0 in /Users/kiperia/Documents/sample-code/arayaman-data-science/.venv/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp->firecrawl) (3.10)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic->firecrawl)\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.33.2 (from pydantic->firecrawl)\n",
            "  Downloading pydantic_core-2.33.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/kiperia/Documents/sample-code/arayaman-data-science/.venv/lib/python3.12/site-packages (from pydantic->firecrawl) (4.13.2)\n",
            "Collecting typing-inspection>=0.4.0 (from pydantic->firecrawl)\n",
            "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kiperia/Documents/sample-code/arayaman-data-science/.venv/lib/python3.12/site-packages (from requests->firecrawl) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kiperia/Documents/sample-code/arayaman-data-science/.venv/lib/python3.12/site-packages (from requests->firecrawl) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/kiperia/Documents/sample-code/arayaman-data-science/.venv/lib/python3.12/site-packages (from requests->firecrawl) (2025.4.26)\n",
            "Using cached firecrawl-2.5.4-py3-none-any.whl (240 kB)\n",
            "Downloading aiohttp-3.11.18-cp312-cp312-macosx_11_0_arm64.whl (457 kB)\n",
            "Downloading multidict-6.4.3-cp312-cp312-macosx_11_0_arm64.whl (37 kB)\n",
            "Downloading yarl-1.20.0-cp312-cp312-macosx_11_0_arm64.whl (95 kB)\n",
            "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
            "Downloading frozenlist-1.6.0-cp312-cp312-macosx_11_0_arm64.whl (121 kB)\n",
            "Downloading propcache-0.3.1-cp312-cp312-macosx_11_0_arm64.whl (46 kB)\n",
            "Downloading pydantic-2.11.4-py3-none-any.whl (443 kB)\n",
            "Downloading pydantic_core-2.33.2-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Using cached websockets-15.0.1-cp312-cp312-macosx_11_0_arm64.whl (173 kB)\n",
            "Installing collected packages: websockets, typing-inspection, python-dotenv, pydantic-core, propcache, multidict, frozenlist, attrs, annotated-types, aiohappyeyeballs, yarl, pydantic, aiosignal, aiohttp, firecrawl\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/15\u001b[0m [firecrawl]15\u001b[0m [firecrawl]\n",
            "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 annotated-types-0.7.0 attrs-25.3.0 firecrawl-2.5.4 frozenlist-1.6.0 multidict-6.4.3 propcache-0.3.1 pydantic-2.11.4 pydantic-core-2.33.2 python-dotenv-1.1.0 typing-inspection-0.4.0 websockets-15.0.1 yarl-1.20.0\n"
          ]
        }
      ],
      "source": [
        "!pip install bs4\n",
        "!pip install sqlalchemy\n",
        "!pip install requests\n",
        "!pip install pandas\n",
        "!pip install firecrawl \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNSaV25EV4By"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from firecrawl import FirecrawlApp\n",
        "app = FirecrawlApp(api_key='fc-e6d03fc937144bc2a873053cfecd5987')\n",
        "target_url = 'https://www.indeed.com/jobs?q=financial+analyst&l=&from=searchOnHP&vjk=6404f31c2a94102c'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPiPb7UnWDXs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Company Location: Arrivia, Inc.Scottsdale, AZ\n",
            "First Job Title: Financial Analyst\n",
            "Link: https://www.indeed.comhttps://www.indeed.com/rc/clk?jk=ddfb81ba0e36015e&bb=aP_xwJufGSlv-vh_kM2-3oK67dsFSTGHQrE0sHmtY0-Cu3pefT-A0_pMIQeSohfoCgZOpC4b9JeYuc7y9waQL6u5rWhwhGvxXeAiYKrKL_tprBQsWFN59GIphsnZ6IhZcYbKnPew_MdziX--C9EKCNhRkpqYiag2&xkcb=SoCA67M3yiFJojSNhR0LbzkdCdPP&fccid=79927877134dd625&vjs=3\n"
          ]
        }
      ],
      "source": [
        "# Scrape the raw HTML\n",
        "scrape_result = app.scrape_url(target_url, formats=['html'])\n",
        "\n",
        "\n",
        "# Access the HTML content from the response object\n",
        "html_content = scrape_result.html  # ✅ This is the correct attribute\n",
        "\n",
        "# Parse with BeautifulSoup\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "# Find the first job listing\n",
        "first_job = soup.select_one('a[data-jk]')\n",
        "company_location = soup.select_one('div.company_location')\n",
        "\n",
        "\n",
        "# Extract data\n",
        "if first_job:\n",
        "    job_title = first_job.get_text(strip=True)\n",
        "    job_href = first_job.get('href')\n",
        "    full_link = f\"https://www.indeed.com{job_href}\"\n",
        "    company_location_text = company_location.get_text(strip=True) if company_location else \"N/A\"\n",
        "    # Print the extracted data\n",
        "    print(\"Company Location:\", company_location_text)\n",
        "\n",
        "    print(\"First Job Title:\", job_title)\n",
        "    print(\"Link:\", full_link)\n",
        "else:\n",
        "    print(\"No job found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJeVBybyWGVw",
        "outputId": "483b32ab-6baf-4062-b650-7de2f1301fa0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from sqlalchemy import create_engine, text\n",
        "from sqlalchemy.exc import SQLAlchemyError\n",
        "\n",
        "# Load credentials from .env\n",
        "load_dotenv()\n",
        "\n",
        "# Database connection info\n",
        "DB_HOST = os.getenv(\"DB_HOST\")\n",
        "DB_PORT = os.getenv(\"DB_PORT\")\n",
        "DB_NAME = os.getenv(\"DB_NAME\")\n",
        "DB_USER = os.getenv(\"DB_USER\")\n",
        "DB_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
        "\n",
        "# Create SQLAlchemy engine\n",
        "engine = create_engine(\n",
        "    f\"postgresql+psycopg2://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
        ")\n",
        "\n",
        "# SQL statements\n",
        "CREATE_TABLE_QUERY = \"\"\"\n",
        "CREATE TABLE IF NOT EXISTS indeed_jobs (\n",
        "    id SERIAL PRIMARY KEY,\n",
        "    url TEXT,\n",
        "    job_title TEXT,\n",
        "    job_location TEXT\n",
        ");\n",
        "\"\"\"\n",
        "\n",
        "INSERT_QUERY = \"\"\"\n",
        "INSERT INTO indeed_jobs (\n",
        "    id, url, job_title, job_location\n",
        ")\n",
        "VALUES (\n",
        "    :id, :url, :jobTitle, :jobLocation\n",
        ") \n",
        "RETURNING id;\n",
        "\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Execute DB operations\n",
        "try:\n",
        "    with engine.begin() as conn:  # automatically commits or rolls back\n",
        "        conn.execute(text(CREATE_TABLE_QUERY))\n",
        "        job = {}\n",
        "        job[\"jobTitle\"] = job_title\n",
        "        job[\"jobLocation\"] = company_location_text\n",
        "        job[\"url\"] = full_link\n",
        "            \n",
        "        conn.execute(text(INSERT_QUERY), job)\n",
        "\n",
        "        print(f\"{job_title} inserted successfully.\")\n",
        "\n",
        "except SQLAlchemyError as e:\n",
        "    print(\"Database error:\", e)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
